{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hooroobaby/NCUAICourse/blob/main/StockRNN_107403037.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRAa8Hh-d4F3"
      },
      "source": [
        "![](https://i.imgur.com/3twMqGw.png)\n",
        "\n",
        "# 作業2 : 股票預測\n",
        "\n",
        "## **請挑選一家公司來做預測**\n",
        "\n",
        "資料集: [鉅亨網](https://www.cnyes.com/)\n",
        "\n",
        "> **資料集說明**\n",
        "\n",
        "別人恐懼我貪婪，一張不賣奇蹟來。\n",
        "\n",
        ">**作業說明**\n",
        "\n",
        "**第一步驟**: 左上角 檔案->在雲端硬碟建立副本。\n",
        "\n",
        "**第二步驟**: 右上角 共用->取得連結->任何知道這個連結的人都能編輯\n",
        "\n",
        "**第三步驟**: 左上角 執行階段->更變執行類型->GPU。 可以加快模型速度\n",
        "\n",
        "**第四步驟**: 檔案名稱，請填寫你的學號，把副本上下引號刪掉\n",
        "\n",
        "請 **隨意修改** 這個colab的程式來完成訓練RNN模型。**不必一定要照著todo提示走。**\n",
        "繳交時記得改檔案標題的學號。\n",
        "\n",
        "這次作業的重點在於RNN的維度問題。\n",
        "\n",
        "> **作業限制**\n",
        "\n",
        "1. 不要動資料集、不要在訓練時偷看test data\n",
        "2. 模型請手刻、手疊，**不要用Transfer Learning**，你可以手刻transformer。\n",
        "3. **不要抄襲**\n",
        "\n",
        "> **作業繳交**\n",
        "\n",
        "1. 完成訓練後保存output結果，更改**檔名學號**，左上角 檔案->下載 成ipynb檔。\n",
        "2. 交一個**pdf**檔，裡面需要附上**你的作業colab連結(設為可編輯、所有人觀看)**、姓名學號年級、過程和執行結果的截圖，並說明你程式撰寫的過程、本次作業心得(字數不限)。\n",
        "3. 上述三個檔案繳交至ee-class。(Text Generation、Stock、PDF)\n",
        "\n",
        "> **繳交期限**\n",
        "\n",
        "1. 電子商務技術(IM5002): 期限至2021/5/5(三) 23:59分\n",
        "2. 人工智慧與機器學習(IM3078): 期限至2021/5/5(三) 23:59分\n",
        "3. 電子商務智慧技術(IMA0200): 期限至2021/5/16(六) 23:59分\n",
        "\n",
        "> **算分標準**\n",
        "\n",
        "Text Generation(30) + Stock RNN(40) + Predict function(20分) + 10分文件\n",
        "1.  [Text Generation](https://colab.research.google.com/drive/1bMkAUaiivMaOr8Sd1Ui-KJeoFImEVUqA?usp=sharing) 換個其他資料集來訓練，完成就30分\n",
        "2. Stock RNN完成模型就40分\n",
        "3. predit function有寫出來就20分(第七段)\n",
        "4. 10分是文件分數\n",
        "5. 記得把以上評分標準截圖、並與你的心得一起貼上pdf\n",
        "\n",
        "有問題再來信助教: lufor129@g.ncu.edu.tw\n",
        "\n",
        "或是每個禮拜一 12:00~14:00到志希館313找助教\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RURvH0RWNsOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977d020d-7978-4dcc-fc0a-a88cbb812150"
      },
      "source": [
        "import os\n",
        "\n",
        "if(os.path.isfile(\"./train.csv\") == False):\n",
        "  !wget -O train.csv \"http://140.115.82.54/NN/Recurrent/train.csv\"\n",
        "  !wget -O test.csv \"http://140.115.82.54/NN/Recurrent/test.csv\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-15 18:12:25--  http://140.115.82.54/NN/Recurrent/train.csv\n",
            "Connecting to 140.115.82.54:80... ^C\n",
            "--2022-05-15 18:14:12--  http://140.115.82.54/NN/Recurrent/test.csv\n",
            "Connecting to 140.115.82.54:80... ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AopvjZOxYmky"
      },
      "source": [
        "## 1. 讀入Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Z6H5ibOZg0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yeQBU3aYqzg"
      },
      "source": [
        "## 2. 取得資料集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF1xYGzJOiQJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "2a2084e0-66ae-4ea6-d7f2-6ed600b72187"
      },
      "source": [
        "train_df = pd.read_csv(\"./train.csv\")\n",
        "train_df = train_df[::-1]\n",
        "print(\"company : \",train_df[\"company\"].unique())\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EmptyDataError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-95526ae33ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"company : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"company\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx0PFFHP3O2d"
      },
      "source": [
        "test_df = pd.read_csv(\"./test.csv\")\n",
        "test_df = test_df[::-1]\n",
        "print(\"test data shape : \",test_df.shape)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4qpw9ZaOkIq"
      },
      "source": [
        "####################################\n",
        "#                 #\n",
        "# 自己決定三家公司哪家公司(填數字)#\n",
        "# 挑選一家公司來做訓練、預測   #\n",
        "#                 #\n",
        "#                 #\n",
        "####################################\n",
        "companys = {\"2330\":\"TSMC\",\"2454\":\"MEDIATEK\",\"2317\":\"FOXCONN\"}\n",
        "company_id = 2330"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qkjc3kHOoD8"
      },
      "source": [
        "company = train_df[train_df[\"company\"] == company_id]\n",
        "company.index = company[\"date\"]\n",
        "company.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LacIURDSOos0"
      },
      "source": [
        "def include_last_iter(start,end,step):\n",
        "  i = start\n",
        "  while i < end:\n",
        "      yield i\n",
        "      i += step\n",
        "  yield end-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztReqaKGOpjl"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "for index,c in enumerate(companys):\n",
        "  plt.subplot(2,2,index+1)\n",
        "  company_df = train_df[train_df[\"company\"] == int(c)]\n",
        "  plt.plot(company_df[\"date\"],company_df[\"price\"])\n",
        "  plt.xticks(list(include_last_iter(0,company_df.shape[0],250)),rotation=30)\n",
        "  plt.title(companys[c])\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"price\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-fbgns7YudB"
      },
      "source": [
        "## 3. 資料前處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg8bkUoMOsxG"
      },
      "source": [
        "# 切分training data & validation data\n",
        "total_len = company.shape[0]\n",
        "training_len = 2100\n",
        "validation_data = total_len - training_len\n",
        "\n",
        "print(\"總共有 : \",total_len,\"筆\")\n",
        "print(\"訓練用 : (前)\",training_len,\"筆\")\n",
        "print(\"驗證用 : (後)\",validation_data,\"筆\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndsIf36gW0Wl"
      },
      "source": [
        "# 自己做 MinMax Transformer 前處理\n",
        "class MinMaxTransformer:\n",
        "  def __init__(self):\n",
        "    self.min = None\n",
        "    self.max = None\n",
        "    self.distance = None\n",
        "\n",
        "  def fit(self,data):\n",
        "    self.min = min(data)\n",
        "    self.max = max(data)\n",
        "    self.distance = self.max - self.min\n",
        "    out = [(d-self.min)/self.distance for d in data]\n",
        "    return out\n",
        "\n",
        "  def recover(self,data):\n",
        "    out = [d*self.distance+self.min for d in data]\n",
        "    return out\n",
        "\n",
        "  def transform(self,data):\n",
        "    out = [(d-self.min)/self.distance for d in data]\n",
        "    return out\n",
        "\n",
        "  def transform_one(self,data):\n",
        "    return (data-self.min)/self.distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urZjG_kDOuTP"
      },
      "source": [
        "train_prices = company[\"price\"].values\n",
        "\n",
        "scalar = MinMaxTransformer()\n",
        "scalar_data = scalar.fit(train_prices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRXJt6sWizwT"
      },
      "source": [
        "# 做正規化\n",
        "print(\"origin data : \",train_prices[:5])\n",
        "print(\"transform data : \",scalar_data[:5])\n",
        "print(\"recover data : \",scalar.recover(scalar_data[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcwp01DcVyCv"
      },
      "source": [
        "# 切分資料集\n",
        "training_data = scalar_data[:training_len]\n",
        "validation_data = scalar_data[training_len:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGbPWD0bVT3j"
      },
      "source": [
        "# todo 決定Seq長度\n",
        "seq_len = 18\n",
        "\n",
        "def split_input_target(seq):\n",
        "  input_txt = tf.expand_dims(seq[:-1],-1)\n",
        "  target_txt = tf.expand_dims(seq[1:],-1)\n",
        "  return input_txt,target_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP47J0uvVcix"
      },
      "source": [
        "def make_ds(data):\n",
        "  data_slice = tf.data.Dataset.from_tensor_slices(data)\n",
        "  data_sequence = data_slice.batch(seq_len+1,drop_remainder=True)\n",
        "\n",
        "  dataset = data_sequence.map(split_input_target)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ahh3yHx8f4u"
      },
      "source": [
        "# 製作資料集\n",
        "train_ds = make_ds(training_data)\n",
        "val_ds = make_ds(validation_data)\n",
        "\n",
        "for input_example,target_exaple in train_ds.take(1):\n",
        "  print(\"Input :\", scalar.recover(input_example.numpy()))\n",
        "  print(\"Target:\", scalar.recover(target_exaple.numpy()))\n",
        "  print(\"-\"*50)\n",
        "  print(\"Input :\", input_example.numpy())\n",
        "  print(\"Target:\", target_exaple.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfilEwW9843h"
      },
      "source": [
        "# Batch size\n",
        "# todo 決定Batch大小\n",
        "BATCH_SIZE = 13\n",
        "\n",
        "BUFFER_SIZE = training_len\n",
        "\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSlabzCtaG3Y"
      },
      "source": [
        "## 4. 建立模型\n",
        "\n",
        "提示: Output是一個數值，所以Model output維度為1。\n",
        "\n",
        "* 輸入維度應為: (batch,seq_len,1)\n",
        "\n",
        "* 輸出維度應為: (batch,seq_len,1)\n",
        "\n",
        "**最後一層Dense不要加上任何activation function。**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ-76DoZ9FG2"
      },
      "source": [
        "# todo\n",
        "input_shapes = (seq_len,1)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout\n",
        "from keras.layers import LSTM\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(75, return_sequences=True, input_shape=input_shapes))\n",
        "\n",
        "      # batch_size = input_shapes[0]\n",
        "model.add(LSTM(64,return_sequences=True))\n",
        "# model.add(LSTM(16,return_sequences=True))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "# model.add(Dense(training_len))\n",
        "# model.add(Dense(BATCH_SIZE))\n",
        "model.add(Dense(input_shapes[1]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsa9LxCcaJxN"
      },
      "source": [
        "## 5. 制定訓練計畫並訓練\n",
        "\n",
        "我們做數字相關預測，可以考慮用Mean_Squared_Error\n",
        "\n",
        "公式:\n",
        "$$MSE = \\frac{1}{n} \\sum_{i=1}^N({\\gamma-\\hat{\\gamma}})^2$$\n",
        "\n",
        "\n",
        "提示:\n",
        "不用管公式在幹嘛，compile帶入mean_squared_error。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIBlVYai-ZuU"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "# model.compile\n",
        "model.compile(loss='mean_squared_error',\n",
        "        optimizer='adam',\n",
        "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
        "        )\n",
        "\n",
        "# model.fit\n",
        "history = model.fit(train_ds,epochs=epochs, validation_data=val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fLe9uGZaMBJ"
      },
      "source": [
        "## 6. 評估模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3T68McvCkcl"
      },
      "source": [
        "l1, = plt.plot(history.history['loss'])\n",
        "l2, = plt.plot(history.history['val_loss'])\n",
        "plt.legend(handles=[l1,l2],labels=['loss','val_loss'],loc='best')\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUoUTu8KC_Sq"
      },
      "source": [
        "test_company = test_df[test_df[\"company\"] == company_id]\n",
        "test_company.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaDw00V8Z-Lw"
      },
      "source": [
        "## 7. 做預測 (20分)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFf2lmOFElai"
      },
      "source": [
        "# 預測 Test data\n",
        "# 現在我們取出了選定公司最後seq_len天\n",
        "# 我們模型透過讀取seq_len天後預測出seq_len+1天的價格\n",
        "\n",
        "# 將日期保存在datas中 (list)\n",
        "# 將真實價格保存在real_prices中 (list)\n",
        "# 將預測價格保存在pred_prices中 (list) *注: 請填入經過正規化後的價格\n",
        "\n",
        "# input是一個經過**正規化**的真實價格list，請試著遍歷test_company來填滿上述三個list\n",
        "# 可以TextGeneration的generateWords function，概念一樣，但是這次input請全部都填入\"真實價格\"\n",
        "# 但是這次input請append \"真實價格\"\n",
        "# 但是這次input請append \"真實價格\"\n",
        "# 但是這次input請append \"真實價格\"\n",
        "# 但是這次input請append \"真實價格\"\n",
        "# 但是這次input請append \"真實價格\"\n",
        "\n",
        "init_price = company.iloc[-seq_len:,:][\"price\"].values\n",
        "dates = []\n",
        "real_prices = []\n",
        "pred_prices = []\n",
        "input = scalar.transform(init_price)\n",
        "\n",
        "for index,row in test_company.iterrows():\n",
        "  real_price = row[\"price\"]\n",
        "  real_prices.append(real_price)\n",
        "  date = row[\"date\"]\n",
        "  dates.append(date)\n",
        "\n",
        "  next_input = tf.expand_dims(input,axis=0)\n",
        "  next_input = np.reshape(next_input,(-1,next_input.shape[1],1))\n",
        "  predicts = model(next_input)\n",
        "  predicts = predicts[0,-1,0]\n",
        "  result = tf.squeeze(predicts).numpy()\n",
        "  pred_prices.append(result)\n",
        "\n",
        "  input.append(scalar.transform_one(real_price)) #4\n",
        "  # input.pop(0)\n",
        "\n",
        "\n",
        "  # 參考步驟 (在for迴圈中) :\n",
        "  # 1. 把input轉**維度**成一個新變數next_input丟入model\n",
        "  # 2. 獲得的pred取出seq中最後一個時間點的output數值 (提示: [0,-1,0])\n",
        "  # 3. 把pred丟入pred_prices中\n",
        "  # 4. 把real_price經過transform_one轉換成小數點並加在input後面\n",
        "  # 5. 保持input長度為seq_len長 (也就是把最前面的數字踢掉)\n",
        "  ##########\n",
        "  # todo #\n",
        "  ##########\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TIRXmjWbbXq"
      },
      "source": [
        "print(f\"dates : {dates[:10]}\")\n",
        "print(f\"real_prices : {real_prices[:10]}\")\n",
        "print(f\"pred_prices : {pred_prices[:10]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cukTAdbxbGKN"
      },
      "source": [
        "* 預期結果:\n",
        "\n",
        "![](https://i.imgur.com/iXdMWD6.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAxyVvCsO1Ei"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "l1, = plt.plot(dates,real_prices)\n",
        "l2, = plt.plot(dates,scalar.recover(pred_prices))\n",
        "plt.xticks(list(include_last_iter(0,len(dates),3)),rotation=30)\n",
        "plt.legend(handles=[l1,l2],labels=['Real','Pred'],loc='best')\n",
        "plt.title(companys[str(company_id)])\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXrQ1B1majO2"
      },
      "source": [
        "* 預期成果\n",
        "\n",
        "照理來說不會100%猜中，猜中幾乎100%可能是不小心偷看答案了。\n",
        "\n",
        "不過要是你的模型正確的話，應該會看到如下pred price在true price附近而不是躺在地上。\n",
        "\n",
        "![](https://i.imgur.com/tFoKEyK.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfTyTnnLuzGU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}